{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9faaffa3-a800-48e3-bd81-e37bddfa28b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Define the storage account and container names\n",
    "storage_account_name = \"health1care1gen2stg\"\n",
    "src_container_name = \"sliver\"\n",
    "tgt_container_name = \"gold\"\n",
    "access_key = \"DQsMJ8waN/6oonXhT8JihU00r+aLJ7BDDF2kGffskG+r0GBqNsH7vSHsFc31hkRSZNdDsn03zIzf+AStGUeVoQ==\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3031d8b8-bc4a-429d-af25-62f6f435d6bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure the mount point for source data\n",
    "# dbutils.fs.mount(\n",
    "#     source = f\"wasbs://{src_container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "#     mount_point = f\"/mnt/{storage_account_name}/{src_container_name}\",\n",
    "#     extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": access_key}\n",
    "# )\n",
    "\n",
    "# Configure the mount point for target data\n",
    "# dbutils.fs.mount(\n",
    "#     source = f\"wasbs://{tgt_container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "#     mount_point = f\"/mnt/{storage_account_name}/{tgt_container_name}\",\n",
    "#     extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": access_key}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa50a01-b0d4-4c2b-9088-a93dd0f72519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_cnt_path = f\"/mnt/{storage_account_name}/{src_container_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6840e1-86ec-4fb3-ad7f-1b63e7f0f3dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"disease\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "disease_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(disease_df)\n",
    "\n",
    "# # Define the path where you want to store your data in Delta format\n",
    "# disease_delta_path = f\"/mnt/{storage_account_name}/{tgt_container_name}/{topic}\"\n",
    "\n",
    "# # Write the DataFrame to the specified path in Delta format\n",
    "# disease_df.write.format(\"delta\").mode(\"overwrite\").save(disease_delta_path)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "disease_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd28613-d505-472a-9417-740576f9a866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"claims\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "claims_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "claims_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e19b541-e6fd-4236-bd18-c5d713bdb140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"group\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "group_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "group_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa518302-dc2e-47f6-a0e9-079f1d3768e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"grpsubgrp\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "grpsubgrp_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "grpsubgrp_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dec734-f25b-453b-86ab-2240d7f9a870",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"hospital\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "hospital_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "hospital_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f56f0e-bfe6-4b9a-bcf5-4b7b059b2f42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"patient\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "patient_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "patient_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae65dd97-0820-497b-b35f-6f93f08a3ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"subgroup\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "subgroup_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "subgroup_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb947d7-c367-4d41-b03a-4dc936c58c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "topic = \"subscriber\"\n",
    "directory_path = f\"{src_cnt_path}/{topic}/\"\n",
    "hive_tabele_name = f\"health_care_hive_met.hlt_slv_{topic}_managed\"\n",
    "\n",
    "# List all files in the directory and sort them by modification time in descending order\n",
    "file_list = [file for file in dbutils.fs.ls(directory_path) if file.name.endswith('.parquet')]\n",
    "\n",
    "# Find the latest file\n",
    "latest_file = sorted(file_list)[0].path\n",
    "\n",
    "# Read the latest Parquet file into a DataFrame\n",
    "subscriber_df: DataFrame = spark.read.format(\"parquet\").load(latest_file)\n",
    "\n",
    "# Write the DataFrame to a managed Hive table\n",
    "subscriber_df.write.mode(\"overwrite\").saveAsTable(hive_tabele_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1473839091490218,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "health-care-load-hive-tables-nb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
